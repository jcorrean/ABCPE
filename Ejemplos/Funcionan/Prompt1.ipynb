{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sGckR_Zu-_qe"
      },
      "outputs": [],
      "source": [
        "from googlesearch import search\n",
        "import csv\n",
        "\n",
        "query = \"graduate programs Universidad Autónoma de México\"\n",
        "\n",
        "urls = []\n",
        "\n",
        "for j in search(query, tld=\"co.in\", num=10, stop=10, pause=2):\n",
        "    urls.append(j)\n",
        "\n",
        "with open('mexico_university_urls.csv', 'w') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([\"URL\"])\n",
        "    writer.writerows([[url] for url in urls])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This uses the googlesearch Python module to programmatically search Google and extract the URLs from the top 10 search results.\n",
        "\n",
        "A few things to note:\n",
        "\n",
        "The googlesearch module may fail or return errors sometimes as it's scraping Google. An API would provide a more reliable solution.\n",
        "\n",
        "Specifying the tld='co.in' limits results to .in domains to avoid international results. You can remove that parameter.\n",
        "\n",
        "The num=10 and stop=10 limits it to the first 10 results. Increase as needed.\n",
        "\n",
        "Adding a pause gives Google time to render pages between requests to avoid getting blocked.\n",
        "\n",
        "Let me know if you have any other questions! Scraping like this provides an alternative without needing API keys."
      ],
      "metadata": {
        "id": "97KmF4QGH9x3"
      }
    }
  ]
}